==> Creating result directory ...
==> Loading test settings ...
==> Deciding torch device ...
==> torch device:  cuda
==> Seeding everything ...
==> Building dynamical system ...
==> Building dynamics neural network ...
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
FullyConnectedNetwork                    [1, 2]                    --
├─Sequential: 1-1                        [1, 2]                    --
│    └─LinearLayer: 2-1                  [1, 16]                   80
│    └─LinearLayer: 2-2                  [1, 16]                   272
│    └─LinearLayer: 2-3                  [1, 2]                    34
├─Sequential: 1-2                        [1, 2]                    (recursive)
│    └─LinearLayer: 2-4                  [1, 16]                   (recursive)
│    └─LinearLayer: 2-5                  [1, 16]                   (recursive)
│    └─LinearLayer: 2-6                  [1, 2]                    (recursive)
==========================================================================================
Total params: 386
Trainable params: 386
Non-trainable params: 0
Total mult-adds (M): 0.00
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.00
Params size (MB): 0.00
Estimated Total Size (MB): 0.00
==========================================================================================
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
FullyConnectedNetwork                    [1, 2]                    --
├─Sequential: 1-1                        [1, 2]                    --
│    └─LinearLayer: 2-1                  [1, 16]                   80
│    └─LinearLayer: 2-2                  [1, 16]                   272
│    └─LinearLayer: 2-3                  [1, 2]                    34
==========================================================================================
Total params: 386
Trainable params: 386
Non-trainable params: 0
Total mult-adds (M): 0.00
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.00
Params size (MB): 0.00
Estimated Total Size (MB): 0.00
==========================================================================================
==> Training dynamics neural network ...
==> Plot drift errors (before training)
> (x, theta): mean = 0.7776, max = 1.4059
> (x, dx): mean = 0.0658, max = 0.1787
> (x, dtheta): mean = 0.0640, max = 0.1473
> (theta, dx): mean = 0.7778, max = 1.4346
> (theta, dtheta): mean = 0.7204, max = 1.3897
> (dx, dtheta): mean = 0.0689, max = 0.1692
==> Plot actuation errors (before training)
> (x, theta): mean = 0.2505, max = 0.3667
> (x, dx): mean = 0.2624, max = 0.4414
> (x, dtheta): mean = 0.2479, max = 0.5105
> (theta, dx): mean = 0.2655, max = 0.3900
> (theta, dtheta): mean = 0.2494, max = 0.4889
> (dx, dtheta): mean = 0.2631, max = 0.5401
==> Train dynamics
Dataset size: 72552
Epoch: 001 | Train Loss: 3.6872E+00 | Drift GN: 6.7568E-01 | Actu. GN: 2.3228E+01 | Time: 839ms
> Save model at epoch 001 with loss 3.6872E+00
> Save model at epoch 002 with loss 2.1525E-01
> Save model at epoch 003 with loss 5.0976E-02
> Save model at epoch 004 with loss 4.7065E-02
> Save model at epoch 005 with loss 4.0754E-02
Epoch: 006 | Train Loss: 3.8685E-02 | Drift GN: 2.9370E-02 | Actu. GN: 5.0321E-01 | Time: 738ms
> Save model at epoch 006 with loss 3.8685E-02
> Save model at epoch 007 with loss 3.6759E-02
> Save model at epoch 008 with loss 3.5132E-02
> Save model at epoch 009 with loss 3.3264E-02
> Save model at epoch 010 with loss 3.2108E-02
Epoch: 011 | Train Loss: 3.1987E-02 | Drift GN: 2.4472E-02 | Actu. GN: 2.1425E-01 | Time: 723ms
> Save model at epoch 011 with loss 3.1987E-02
Epoch: 016 | Train Loss: 3.2357E-02 | Drift GN: 3.1771E-02 | Actu. GN: 2.8805E-01 | Time: 756ms
> Save model at epoch 019 with loss 3.1817E-02
Epoch: 021 | Train Loss: 3.9669E-01 | Drift GN: 1.3768E-01 | Actu. GN: 4.3306E+00 | Time: 703ms
Epoch: 026 | Train Loss: 3.4725E-02 | Drift GN: 3.3766E-02 | Actu. GN: 3.5289E-01 | Time: 705ms
Epoch: 031 | Train Loss: 1.0041E+00 | Drift GN: 3.1132E-01 | Actu. GN: 1.5102E+01 | Time: 748ms
Epoch: 036 | Train Loss: 4.2421E-01 | Drift GN: 9.6803E-02 | Actu. GN: 1.0258E+01 | Time: 699ms
Epoch: 041 | Train Loss: 3.3215E-02 | Drift GN: 3.2790E-02 | Actu. GN: 2.9914E-01 | Time: 762ms
Epoch: 046 | Train Loss: 3.3827E-02 | Drift GN: 4.8872E-02 | Actu. GN: 4.4215E-01 | Time: 720ms
> Save model at epoch 049 with loss 3.1619E-02
> Save model at epoch 050 with loss 3.1396E-02
Epoch: 051 | Train Loss: 3.1767E-02 | Drift GN: 3.3493E-02 | Actu. GN: 2.9169E-01 | Time: 734ms
Epoch: 056 | Train Loss: 3.1374E-02 | Drift GN: 3.2511E-02 | Actu. GN: 2.4857E-01 | Time: 773ms
> Save model at epoch 056 with loss 3.1374E-02
> Save model at epoch 058 with loss 3.1178E-02
Epoch: 061 | Train Loss: 3.1752E-02 | Drift GN: 3.7586E-02 | Actu. GN: 3.1244E-01 | Time: 709ms
> Save model at epoch 062 with loss 3.1059E-02
> Save model at epoch 063 with loss 3.1034E-02
> Save model at epoch 064 with loss 3.0956E-02
Epoch: 066 | Train Loss: 3.0679E-02 | Drift GN: 2.9961E-02 | Actu. GN: 1.7006E-01 | Time: 734ms
> Save model at epoch 066 with loss 3.0679E-02
Epoch: 071 | Train Loss: 3.0608E-02 | Drift GN: 2.8280E-02 | Actu. GN: 1.8708E-01 | Time: 699ms
> Save model at epoch 071 with loss 3.0608E-02
> Save model at epoch 073 with loss 3.0561E-02
> Save model at epoch 074 with loss 3.0504E-02
> Save model at epoch 075 with loss 3.0360E-02
Epoch: 076 | Train Loss: 3.0412E-02 | Drift GN: 2.7093E-02 | Actu. GN: 1.6102E-01 | Time: 808ms
> Save model at epoch 080 with loss 3.0331E-02
Epoch: 081 | Train Loss: 3.0326E-02 | Drift GN: 2.6649E-02 | Actu. GN: 1.5613E-01 | Time: 718ms
> Save model at epoch 081 with loss 3.0326E-02
> Save model at epoch 084 with loss 3.0244E-02
> Save model at epoch 085 with loss 3.0176E-02
Epoch: 086 | Train Loss: 3.0138E-02 | Drift GN: 2.3313E-02 | Actu. GN: 1.1765E-01 | Time: 716ms
> Save model at epoch 086 with loss 3.0138E-02
> Save model at epoch 087 with loss 3.0133E-02
> Save model at epoch 088 with loss 3.0081E-02
> Save model at epoch 090 with loss 3.0011E-02
Epoch: 091 | Train Loss: 2.9998E-02 | Drift GN: 2.1692E-02 | Actu. GN: 8.9261E-02 | Time: 748ms
> Save model at epoch 091 with loss 2.9998E-02
> Save model at epoch 094 with loss 2.9961E-02
> Save model at epoch 095 with loss 2.9954E-02
Epoch: 096 | Train Loss: 2.9962E-02 | Drift GN: 2.1030E-02 | Actu. GN: 8.0887E-02 | Time: 686ms
> Save model at epoch 097 with loss 2.9941E-02
> Save model at epoch 098 with loss 2.9931E-02
> Save model at epoch 099 with loss 2.9919E-02
> Save model at epoch 100 with loss 2.9916E-02
Total time: 1m13s
==> Plot drift errors (after training)
> (x, theta): mean = 0.2902, max = 0.4703
> (x, dx): mean = 0.1703, max = 0.3124
> (x, dtheta): mean = 0.2218, max = 0.3933
> (theta, dx): mean = 0.2569, max = 0.4743
> (theta, dtheta): mean = 0.1884, max = 0.5083
> (dx, dtheta): mean = 0.2274, max = 0.5217
==> Plot actuation errors (after training)
> (x, theta): mean = 0.0186, max = 0.0273
> (x, dx): mean = 0.0273, max = 0.0273
> (x, dtheta): mean = 0.0273, max = 0.0273
> (theta, dx): mean = 0.0186, max = 0.0273
> (theta, dtheta): mean = 0.0186, max = 0.0273
> (dx, dtheta): mean = 0.0273, max = 0.0273
==> Building Lyapunov neural network ...
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
LyapunovNetwork                          [1, 1]                    --
├─Sequential: 1-1                        [1, 64]                   --
│    └─LinearLayer: 2-1                  [1, 64]                   256
│    └─LinearLayer: 2-2                  [1, 64]                   4,096
│    └─LinearLayer: 2-3                  [1, 64]                   4,096
==========================================================================================
Total params: 8,448
Trainable params: 8,448
Non-trainable params: 0
Total mult-adds (M): 0.01
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.00
Params size (MB): 0.03
Estimated Total Size (MB): 0.04
==========================================================================================
==> Building controller neural network ...
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
FullyConnectedNetwork                    [1, 1]                    --
├─Sequential: 1-1                        [1, 1]                    --
│    └─LinearLayer: 2-1                  [1, 16]                   64
│    └─LinearLayer: 2-2                  [1, 16]                   256
│    └─LinearLayer: 2-3                  [1, 1]                    16
==========================================================================================
Total params: 336
Trainable params: 336
Non-trainable params: 0
Total mult-adds (M): 0.00
==========================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 0.00
Params size (MB): 0.00
Estimated Total Size (MB): 0.00
==========================================================================================
==> Amount of training data:  10000
==> Creating training data ...
==> Start training ...
Epoch: 001 | Train Loss: 1.3777E-02 | Lya GN: 4.6072E+00 | Ctrl GN: 6.8441E-02 | Time: 82ms
Epoch: 001 | Test Loss: 1.3394E-02 | Time: 25ms
> Save at epoch 001 | Test loss 1.3394E-02
Epoch: 002 | Test Loss: 1.3215E-02 | Time: 24ms
> Save at epoch 002 | Test loss 1.3215E-02
Epoch: 003 | Test Loss: 1.3056E-02 | Time: 25ms
> Save at epoch 003 | Test loss 1.3056E-02
Epoch: 004 | Test Loss: 1.2721E-02 | Time: 25ms
> Save at epoch 004 | Test loss 1.2721E-02
Epoch: 005 | Test Loss: 1.2235E-02 | Time: 25ms
> Save at epoch 005 | Test loss 1.2235E-02
Epoch: 006 | Train Loss: 1.2465E-02 | Lya GN: 4.4346E+00 | Ctrl GN: 6.6288E-02 | Time: 58ms
Epoch: 006 | Test Loss: 1.1766E-02 | Time: 25ms
> Save at epoch 006 | Test loss 1.1766E-02
Epoch: 007 | Test Loss: 1.1149E-02 | Time: 25ms
> Save at epoch 007 | Test loss 1.1149E-02
Epoch: 008 | Test Loss: 1.0539E-02 | Time: 25ms
> Save at epoch 008 | Test loss 1.0539E-02
Epoch: 009 | Test Loss: 9.8467E-03 | Time: 25ms
> Save at epoch 009 | Test loss 9.8467E-03
Epoch: 010 | Test Loss: 9.0684E-03 | Time: 25ms
> Save at epoch 010 | Test loss 9.0684E-03
Epoch: 011 | Train Loss: 8.9325E-03 | Lya GN: 3.7295E+00 | Ctrl GN: 5.5177E-02 | Time: 58ms
Epoch: 011 | Test Loss: 8.2994E-03 | Time: 25ms
> Save at epoch 011 | Test loss 8.2994E-03
Epoch: 012 | Test Loss: 7.5005E-03 | Time: 25ms
> Save at epoch 012 | Test loss 7.5005E-03
Epoch: 013 | Test Loss: 6.6063E-03 | Time: 25ms
> Save at epoch 013 | Test loss 6.6063E-03
Epoch: 014 | Test Loss: 5.8080E-03 | Time: 25ms
> Save at epoch 014 | Test loss 5.8080E-03
Epoch: 015 | Test Loss: 5.0864E-03 | Time: 25ms
> Save at epoch 015 | Test loss 5.0864E-03
Epoch: 016 | Train Loss: 4.8611E-03 | Lya GN: 2.5393E+00 | Ctrl GN: 3.6657E-02 | Time: 58ms
Epoch: 016 | Test Loss: 4.3169E-03 | Time: 25ms
> Save at epoch 016 | Test loss 4.3169E-03
Epoch: 017 | Test Loss: 3.6210E-03 | Time: 25ms
> Save at epoch 017 | Test loss 3.6210E-03
Epoch: 018 | Test Loss: 2.9309E-03 | Time: 25ms
> Save at epoch 018 | Test loss 2.9309E-03
Epoch: 019 | Test Loss: 2.2716E-03 | Time: 25ms
> Save at epoch 019 | Test loss 2.2716E-03
Epoch: 020 | Test Loss: 1.7442E-03 | Time: 55ms
> Save at epoch 020 | Test loss 1.7442E-03
Epoch: 021 | Train Loss: 1.5903E-03 | Lya GN: 1.4185E+00 | Ctrl GN: 2.0769E-02 | Time: 58ms
Epoch: 021 | Test Loss: 1.2764E-03 | Time: 25ms
> Save at epoch 021 | Test loss 1.2764E-03
Epoch: 022 | Test Loss: 8.4591E-04 | Time: 25ms
> Save at epoch 022 | Test loss 8.4591E-04
Epoch: 023 | Test Loss: 5.1964E-04 | Time: 25ms
> Save at epoch 023 | Test loss 5.1964E-04
Epoch: 024 | Test Loss: 2.8704E-04 | Time: 25ms
> Save at epoch 024 | Test loss 2.8704E-04
Epoch: 025 | Test Loss: 1.2696E-04 | Time: 25ms
> Save at epoch 025 | Test loss 1.2696E-04
Epoch: 026 | Train Loss: 1.0189E-04 | Lya GN: 4.3643E-01 | Ctrl GN: 9.8116E-03 | Time: 58ms
Epoch: 026 | Test Loss: 3.7285E-05 | Time: 25ms
> Save at epoch 026 | Test loss 3.7285E-05
Epoch: 027 | Test Loss: 7.1674E-06 | Time: 25ms
> Save at epoch 027 | Test loss 7.1674E-06
Epoch: 028 | Test Loss: 2.2548E-06 | Time: 56ms
> Save at epoch 028 | Test loss 2.2548E-06
Epoch: 029 | Test Loss: 1.6298E-06 | Time: 26ms
> Save at epoch 029 | Test loss 1.6298E-06
Epoch: 030 | Test Loss: 1.4525E-06 | Time: 26ms
> Save at epoch 030 | Test loss 1.4525E-06
Epoch: 031 | Train Loss: 1.1505E-05 | Lya GN: 1.9623E-02 | Ctrl GN: 4.4303E-04 | Time: 60ms
Epoch: 031 | Test Loss: 1.3306E-06 | Time: 26ms
> Save at epoch 031 | Test loss 1.3306E-06
Epoch: 032 | Test Loss: 1.2259E-06 | Time: 26ms
> Save at epoch 032 | Test loss 1.2259E-06
Epoch: 033 | Test Loss: 1.1206E-06 | Time: 25ms
> Save at epoch 033 | Test loss 1.1206E-06
Epoch: 034 | Test Loss: 1.0720E-06 | Time: 26ms
> Save at epoch 034 | Test loss 1.0720E-06
Epoch: 035 | Test Loss: 9.9993E-07 | Time: 26ms
> Save at epoch 035 | Test loss 9.9993E-07
Epoch: 036 | Train Loss: 1.0161E-05 | Lya GN: 1.2518E-02 | Ctrl GN: 2.5649E-04 | Time: 60ms
Epoch: 036 | Test Loss: 8.5139E-07 | Time: 54ms
> Save at epoch 036 | Test loss 8.5139E-07
Epoch: 037 | Test Loss: 7.2228E-07 | Time: 26ms
> Save at epoch 037 | Test loss 7.2228E-07
Epoch: 038 | Test Loss: 6.3347E-07 | Time: 26ms
> Save at epoch 038 | Test loss 6.3347E-07
Epoch: 039 | Test Loss: 5.5604E-07 | Time: 26ms
> Save at epoch 039 | Test loss 5.5604E-07
Epoch: 040 | Test Loss: 5.1012E-07 | Time: 26ms
> Save at epoch 040 | Test loss 5.1012E-07
Epoch: 041 | Train Loss: 8.8446E-06 | Lya GN: 8.1576E-03 | Ctrl GN: 1.5778E-04 | Time: 60ms
Epoch: 041 | Test Loss: 4.4802E-07 | Time: 26ms
> Save at epoch 041 | Test loss 4.4802E-07
Epoch: 042 | Test Loss: 3.8149E-07 | Time: 26ms
> Save at epoch 042 | Test loss 3.8149E-07
Epoch: 043 | Test Loss: 3.3665E-07 | Time: 26ms
> Save at epoch 043 | Test loss 3.3665E-07
Epoch: 044 | Test Loss: 3.2090E-07 | Time: 26ms
> Save at epoch 044 | Test loss 3.2090E-07
Epoch: 045 | Test Loss: 2.5588E-07 | Time: 26ms
> Save at epoch 045 | Test loss 2.5588E-07
Epoch: 046 | Train Loss: 7.5893E-06 | Lya GN: 1.0533E-02 | Ctrl GN: 2.3470E-04 | Time: 60ms
Epoch: 046 | Test Loss: 2.5493E-07 | Time: 26ms
> Save at epoch 046 | Test loss 2.5493E-07
Epoch: 047 | Test Loss: 1.9576E-07 | Time: 26ms
> Save at epoch 047 | Test loss 1.9576E-07
Epoch: 048 | Test Loss: 1.8184E-07 | Time: 26ms
> Save at epoch 048 | Test loss 1.8184E-07
Epoch: 049 | Test Loss: 1.5848E-07 | Time: 26ms
> Save at epoch 049 | Test loss 1.5848E-07
Epoch: 050 | Test Loss: 1.2624E-07 | Time: 26ms
> Save at epoch 050 | Test loss 1.2624E-07
Epoch: 051 | Train Loss: 6.8809E-06 | Lya GN: 9.6961E-03 | Ctrl GN: 2.2704E-04 | Time: 59ms
Epoch: 051 | Test Loss: 1.0113E-07 | Time: 26ms
> Save at epoch 051 | Test loss 1.0113E-07
Epoch: 052 | Test Loss: 8.3712E-08 | Time: 26ms
> Save at epoch 052 | Test loss 8.3712E-08
Epoch: 053 | Test Loss: 6.9726E-08 | Time: 26ms
> Save at epoch 053 | Test loss 6.9726E-08
Epoch: 054 | Test Loss: 5.4938E-08 | Time: 26ms
> Save at epoch 054 | Test loss 5.4938E-08
Epoch: 055 | Test Loss: 4.7512E-08 | Time: 26ms
> Save at epoch 055 | Test loss 4.7512E-08
Epoch: 056 | Train Loss: 6.4338E-06 | Lya GN: 6.7533E-03 | Ctrl GN: 1.7315E-04 | Time: 60ms
Epoch: 056 | Test Loss: 4.2012E-08 | Time: 26ms
> Save at epoch 056 | Test loss 4.2012E-08
Epoch: 057 | Test Loss: 3.6679E-08 | Time: 26ms
> Save at epoch 057 | Test loss 3.6679E-08
Epoch: 058 | Test Loss: 3.2142E-08 | Time: 26ms
> Save at epoch 058 | Test loss 3.2142E-08
Epoch: 059 | Test Loss: 2.8272E-08 | Time: 26ms
> Save at epoch 059 | Test loss 2.8272E-08
Epoch: 060 | Test Loss: 2.4940E-08 | Time: 26ms
> Save at epoch 060 | Test loss 2.4940E-08
Epoch: 061 | Train Loss: 5.9185E-06 | Lya GN: 5.9041E-03 | Ctrl GN: 1.4267E-04 | Time: 60ms
Epoch: 061 | Test Loss: 2.2280E-08 | Time: 54ms
> Save at epoch 061 | Test loss 2.2280E-08
Epoch: 062 | Test Loss: 2.0444E-08 | Time: 26ms
> Save at epoch 062 | Test loss 2.0444E-08
Epoch: 063 | Test Loss: 1.8739E-08 | Time: 26ms
> Save at epoch 063 | Test loss 1.8739E-08
Epoch: 064 | Test Loss: 1.7239E-08 | Time: 26ms
> Save at epoch 064 | Test loss 1.7239E-08
Epoch: 065 | Test Loss: 1.5983E-08 | Time: 26ms
> Save at epoch 065 | Test loss 1.5983E-08
Epoch: 066 | Train Loss: 5.5524E-06 | Lya GN: 5.6469E-03 | Ctrl GN: 1.4252E-04 | Time: 58ms
Epoch: 066 | Test Loss: 1.4591E-08 | Time: 26ms
> Save at epoch 066 | Test loss 1.4591E-08
Epoch: 067 | Test Loss: 1.3222E-08 | Time: 26ms
> Save at epoch 067 | Test loss 1.3222E-08
Epoch: 068 | Test Loss: 1.1986E-08 | Time: 26ms
> Save at epoch 068 | Test loss 1.1986E-08
Epoch: 069 | Test Loss: 1.0862E-08 | Time: 26ms
> Save at epoch 069 | Test loss 1.0862E-08
Epoch: 070 | Test Loss: 9.7652E-09 | Time: 26ms
> Save at epoch 070 | Test loss 9.7652E-09
Epoch: 071 | Train Loss: 5.2089E-06 | Lya GN: 4.8980E-03 | Ctrl GN: 1.3875E-04 | Time: 57ms
Epoch: 071 | Test Loss: 8.6746E-09 | Time: 26ms
> Save at epoch 071 | Test loss 8.6746E-09
Epoch: 072 | Test Loss: 7.6812E-09 | Time: 26ms
> Save at epoch 072 | Test loss 7.6812E-09
Epoch: 073 | Test Loss: 6.8072E-09 | Time: 26ms
> Save at epoch 073 | Test loss 6.8072E-09
Epoch: 074 | Test Loss: 6.0403E-09 | Time: 26ms
> Save at epoch 074 | Test loss 6.0403E-09
Epoch: 075 | Test Loss: 5.2617E-09 | Time: 26ms
> Save at epoch 075 | Test loss 5.2617E-09
Epoch: 076 | Train Loss: 5.0628E-06 | Lya GN: 4.8243E-03 | Ctrl GN: 1.3760E-04 | Time: 56ms
Epoch: 076 | Test Loss: 4.5307E-09 | Time: 26ms
> Save at epoch 076 | Test loss 4.5307E-09
Epoch: 077 | Test Loss: 3.9764E-09 | Time: 26ms
> Save at epoch 077 | Test loss 3.9764E-09
Epoch: 078 | Test Loss: 3.5684E-09 | Time: 26ms
> Save at epoch 078 | Test loss 3.5684E-09
Epoch: 079 | Test Loss: 3.1174E-09 | Time: 26ms
> Save at epoch 079 | Test loss 3.1174E-09
Epoch: 080 | Test Loss: 2.7413E-09 | Time: 26ms
> Save at epoch 080 | Test loss 2.7413E-09
Epoch: 081 | Train Loss: 4.8996E-06 | Lya GN: 4.9357E-03 | Ctrl GN: 1.3260E-04 | Time: 56ms
Epoch: 081 | Test Loss: 2.3736E-09 | Time: 26ms
> Save at epoch 081 | Test loss 2.3736E-09
Epoch: 082 | Test Loss: 2.0589E-09 | Time: 26ms
> Save at epoch 082 | Test loss 2.0589E-09
Epoch: 083 | Test Loss: 1.7980E-09 | Time: 26ms
> Save at epoch 083 | Test loss 1.7980E-09
Epoch: 084 | Test Loss: 1.6896E-09 | Time: 26ms
> Save at epoch 084 | Test loss 1.6896E-09
Epoch: 085 | Test Loss: 1.5919E-09 | Time: 26ms
> Save at epoch 085 | Test loss 1.5919E-09
Epoch: 086 | Train Loss: 4.9835E-06 | Lya GN: 5.1612E-03 | Ctrl GN: 1.5677E-04 | Time: 56ms
Epoch: 086 | Test Loss: 1.5090E-09 | Time: 54ms
> Save at epoch 086 | Test loss 1.5090E-09
Epoch: 087 | Test Loss: 1.4443E-09 | Time: 26ms
> Save at epoch 087 | Test loss 1.4443E-09
Epoch: 088 | Test Loss: 1.3793E-09 | Time: 26ms
> Save at epoch 088 | Test loss 1.3793E-09
Epoch: 089 | Test Loss: 1.3304E-09 | Time: 26ms
> Save at epoch 089 | Test loss 1.3304E-09
Epoch: 090 | Test Loss: 1.2835E-09 | Time: 26ms
> Save at epoch 090 | Test loss 1.2835E-09
Epoch: 091 | Train Loss: 4.8388E-06 | Lya GN: 4.8393E-03 | Ctrl GN: 1.3467E-04 | Time: 56ms
Epoch: 091 | Test Loss: 1.2478E-09 | Time: 26ms
> Save at epoch 091 | Test loss 1.2478E-09
Epoch: 092 | Test Loss: 1.2196E-09 | Time: 26ms
> Save at epoch 092 | Test loss 1.2196E-09
Epoch: 093 | Test Loss: 1.1964E-09 | Time: 26ms
> Save at epoch 093 | Test loss 1.1964E-09
Epoch: 094 | Test Loss: 1.1816E-09 | Time: 26ms
> Save at epoch 094 | Test loss 1.1816E-09
Epoch: 095 | Test Loss: 1.1705E-09 | Time: 26ms
> Save at epoch 095 | Test loss 1.1705E-09
Epoch: 096 | Train Loss: 4.7676E-06 | Lya GN: 4.6694E-03 | Ctrl GN: 1.4157E-04 | Time: 59ms
Epoch: 096 | Test Loss: 1.1641E-09 | Time: 26ms
> Save at epoch 096 | Test loss 1.1641E-09
Epoch: 097 | Test Loss: 1.1608E-09 | Time: 26ms
> Save at epoch 097 | Test loss 1.1608E-09
Epoch: 098 | Test Loss: 1.1592E-09 | Time: 26ms
> Save at epoch 098 | Test loss 1.1592E-09
Epoch: 099 | Test Loss: 1.1590E-09 | Time: 26ms
> Save at epoch 099 | Test loss 1.1590E-09
Epoch: 100 | Test Loss: 1.1590E-09 | Time: 26ms
> Save at epoch 100 | Test loss 1.1590E-09
Total time: 8s929ms
==> Visualizing training and test losses ...
==> Saving the training results ...
==> Computing the Lyapunov function and stability condition ...
==> Checking the forward invariant set ...
==> Checking the Lyapunov function and stability condition ...
> The stability condition is not satisfied at some points outside the ball.
> The number of bad points:  141
[[ 0.16153846  0.07179487  0.47948719 -0.03333333]
 [ 0.16153846  0.07179487  0.53589745 -0.03333333]
 [ 0.2076923   0.07179487  0.47948719 -0.03333333]
 [ 0.2076923   0.07179487  0.53589745 -0.03333333]
 [ 0.16153846  0.09230769  0.70512822 -0.03333333]
 [ 0.2076923   0.09230769  0.64871796 -0.03333333]
 [ 0.2076923   0.09230769  0.70512822 -0.03333333]
 [ 0.25384615  0.09230769  0.64871796 -0.03333333]
 [ 0.25384615  0.09230769  0.70512822 -0.03333333]
 [ 0.25384615  0.09230769  0.76153848 -0.03333333]
 [ 0.29999999  0.09230769  0.64871796 -0.03333333]
 [ 0.29999999  0.09230769  0.70512822 -0.03333333]
 [ 0.34615384  0.09230769  0.64871796 -0.03333333]
 [ 0.2076923   0.11282051  0.81794874 -0.03333333]
 [ 0.2076923   0.11282051  0.87435899 -0.03333333]
 [ 0.2076923   0.11282051  0.93076925 -0.03333333]
 [ 0.25384615  0.11282051  0.81794874 -0.03333333]
 [ 0.25384615  0.11282051  0.87435899 -0.03333333]
 [ 0.25384615  0.11282051  0.93076925 -0.03333333]
 [ 0.29999999  0.11282051  0.76153848 -0.03333333]
 [ 0.29999999  0.11282051  0.81794874 -0.03333333]
 [ 0.29999999  0.11282051  0.87435899 -0.03333333]
 [ 0.29999999  0.11282051  0.93076925 -0.03333333]
 [ 0.29999999  0.11282051  1.04358977  0.03333333]
 [ 0.29999999  0.11282051  1.10000002  0.03333333]
 [ 0.34615384  0.11282051  0.76153848 -0.03333333]
 [ 0.34615384  0.11282051  0.81794874 -0.03333333]
 [ 0.34615384  0.11282051  0.87435899 -0.03333333]
 [ 0.34615384  0.11282051  0.93076925 -0.03333333]
 [ 0.34615384  0.11282051  0.98717951  0.03333333]
 [ 0.34615384  0.11282051  1.04358977  0.03333333]
 [ 0.34615384  0.11282051  1.10000002  0.03333333]
 [ 0.39230768  0.11282051  0.76153848 -0.03333333]
 [ 0.39230768  0.11282051  0.81794874 -0.03333333]
 [ 0.39230768  0.11282051  0.87435899 -0.03333333]
 [ 0.39230768  0.11282051  0.93076925 -0.03333333]
 [ 0.39230768  0.11282051  0.98717951  0.03333333]
 [ 0.39230768  0.11282051  1.04358977  0.03333333]
 [ 0.39230768  0.11282051  1.10000002  0.03333333]
 [ 0.43846153  0.11282051  0.76153848 -0.03333333]
 [ 0.43846153  0.11282051  0.81794874 -0.03333333]
 [ 0.43846153  0.11282051  0.87435899 -0.03333333]
 [ 0.43846153  0.11282051  0.98717951  0.03333333]
 [ 0.43846153  0.11282051  1.04358977  0.03333333]
 [ 0.48461537  0.11282051  0.98717951  0.03333333]
 [ 0.48461537  0.11282051  1.04358977  0.03333333]
 [ 0.2076923   0.13333334  1.04358977 -0.03333333]
 [ 0.2076923   0.13333334  1.10000002 -0.03333333]
 [ 0.25384615  0.13333334  0.98717951 -0.03333333]
 [ 0.25384615  0.13333334  1.04358977 -0.03333333]
 [ 0.25384615  0.13333334  1.10000002 -0.03333333]
 [ 0.29999999  0.13333334  0.81794874 -0.1       ]
 [ 0.29999999  0.13333334  0.87435899 -0.1       ]
 [ 0.29999999  0.13333334  0.93076925 -0.03333333]
 [ 0.29999999  0.13333334  0.98717951 -0.03333333]
 [ 0.29999999  0.13333334  1.04358977 -0.03333333]
 [ 0.29999999  0.13333334  1.10000002 -0.03333333]
 [ 0.34615384  0.13333334  0.81794874 -0.1       ]
 [ 0.34615384  0.13333334  0.87435899 -0.1       ]
 [ 0.34615384  0.13333334  0.93076925 -0.03333333]
 [ 0.34615384  0.13333334  0.98717951 -0.03333333]
 [ 0.34615384  0.13333334  1.04358977 -0.03333333]
 [ 0.34615384  0.13333334  1.10000002 -0.03333333]
 [ 0.39230768  0.13333334  0.87435899 -0.1       ]
 [ 0.39230768  0.13333334  0.93076925 -0.03333333]
 [ 0.39230768  0.13333334  0.98717951 -0.03333333]
 [ 0.39230768  0.13333334  1.04358977 -0.03333333]
 [ 0.39230768  0.13333334  1.10000002 -0.03333333]
 [ 0.43846153  0.13333334  0.93076925 -0.03333333]
 [ 0.43846153  0.13333334  0.98717951 -0.03333333]
 [ 0.43846153  0.13333334  1.04358977 -0.03333333]
 [ 0.43846153  0.13333334  1.10000002 -0.03333333]
 [ 0.43846153  0.13333334  1.10000002  0.03333333]
 [ 0.48461537  0.13333334  0.93076925 -0.03333333]
 [ 0.48461537  0.13333334  0.98717951 -0.03333333]
 [ 0.48461537  0.13333334  1.04358977 -0.03333333]
 [ 0.48461537  0.13333334  1.10000002 -0.03333333]
 [ 0.48461537  0.13333334  1.10000002  0.03333333]
 [ 0.53076922  0.13333334  0.93076925 -0.03333333]
 [ 0.53076922  0.13333334  0.98717951 -0.03333333]
 [ 0.53076922  0.13333334  1.04358977 -0.03333333]
 [ 0.53076922  0.13333334  1.10000002 -0.03333333]
 [ 0.53076922  0.13333334  1.10000002  0.03333333]
 [ 0.57692306  0.13333334  0.93076925 -0.03333333]
 [ 0.57692306  0.13333334  0.98717951 -0.03333333]
 [ 0.57692306  0.13333334  1.04358977 -0.03333333]
 [ 0.57692306  0.13333334  1.10000002  0.03333333]
 [ 0.62307691  0.13333334  1.10000002  0.03333333]
 [ 0.25384615  0.15384616  1.04358977 -0.1       ]
 [ 0.29999999  0.15384616  0.98717951 -0.1       ]
 [ 0.29999999  0.15384616  1.04358977 -0.1       ]
 [ 0.29999999  0.15384616  1.10000002 -0.1       ]
 [ 0.34615384  0.15384616  0.98717951 -0.1       ]
 [ 0.34615384  0.15384616  1.04358977 -0.1       ]
 [ 0.34615384  0.15384616  1.10000002 -0.1       ]
 [ 0.34615384  0.15384616  1.10000002 -0.03333333]
 [ 0.39230768  0.15384616  0.93076925 -0.1       ]
 [ 0.39230768  0.15384616  0.98717951 -0.1       ]
 [ 0.39230768  0.15384616  1.04358977 -0.1       ]
 [ 0.39230768  0.15384616  1.10000002 -0.1       ]
 [ 0.39230768  0.15384616  1.10000002 -0.03333333]
 [ 0.43846153  0.15384616  0.93076925 -0.1       ]
 [ 0.43846153  0.15384616  0.98717951 -0.1       ]
 [ 0.43846153  0.15384616  1.04358977 -0.1       ]
 [ 0.43846153  0.15384616  1.04358977 -0.03333333]
 [ 0.43846153  0.15384616  1.10000002 -0.1       ]
 [ 0.43846153  0.15384616  1.10000002 -0.03333333]
 [ 0.48461537  0.15384616  0.93076925 -0.1       ]
 [ 0.48461537  0.15384616  0.98717951 -0.1       ]
 [ 0.48461537  0.15384616  1.04358977 -0.1       ]
 [ 0.48461537  0.15384616  1.04358977 -0.03333333]
 [ 0.48461537  0.15384616  1.10000002 -0.1       ]
 [ 0.48461537  0.15384616  1.10000002 -0.03333333]
 [ 0.53076922  0.15384616  0.93076925 -0.1       ]
 [ 0.53076922  0.15384616  0.98717951 -0.1       ]
 [ 0.53076922  0.15384616  1.04358977 -0.1       ]
 [ 0.53076922  0.15384616  1.04358977 -0.03333333]
 [ 0.53076922  0.15384616  1.10000002 -0.1       ]
 [ 0.53076922  0.15384616  1.10000002 -0.03333333]
 [ 0.57692306  0.15384616  0.98717951 -0.1       ]
 [ 0.57692306  0.15384616  1.04358977 -0.1       ]
 [ 0.57692306  0.15384616  1.04358977 -0.03333333]
 [ 0.57692306  0.15384616  1.10000002 -0.03333333]
 [ 0.62307691  0.15384616  1.04358977 -0.03333333]
 [ 0.62307691  0.15384616  1.10000002 -0.03333333]
 [ 0.66923075  0.15384616  1.04358977 -0.03333333]
 [ 0.66923075  0.15384616  1.10000002 -0.03333333]
 [ 0.7153846   0.15384616  1.10000002 -0.03333333]
 [ 0.34615384  0.17435898  1.10000002 -0.1       ]
 [ 0.39230768  0.17435898  1.10000002 -0.1       ]
 [ 0.43846153  0.17435898  1.10000002 -0.1       ]
 [ 0.48461537  0.17435898  1.04358977 -0.1       ]
 [ 0.48461537  0.17435898  1.10000002 -0.1       ]
 [ 0.53076922  0.17435898  1.04358977 -0.1       ]
 [ 0.53076922  0.17435898  1.10000002 -0.1       ]
 [ 0.57692306  0.17435898  1.04358977 -0.1       ]
 [ 0.57692306  0.17435898  1.10000002 -0.1       ]
 [ 0.62307691  0.17435898  1.04358977 -0.1       ]
 [ 0.62307691  0.17435898  1.10000002 -0.1       ]
 [ 0.66923075  0.17435898  1.10000002 -0.1       ]
 [ 0.7153846   0.17435898  1.10000002 -0.1       ]]
> Success:  False
==> Saving the forward invariant set ...
> Forward invariant set Lyapunov value:  0.0019301969
> Forward invariant set percentage:  0.0460125
==> Visualizing the Lyapunov function and stability condition ...
==> Done!
